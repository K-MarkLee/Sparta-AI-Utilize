{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Program\\Anaconda\\envs\\machine-learning\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "# d_model 임배딩 차원의 수/ nhead 헤드의 갯수\n",
    "model = Transformer(d_model = 512, nhead=8,num_encoder_layers=6, num_decoder_layers= 6 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#데이터 차원수가 높기에 lr 을 낮게 설정\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 기준 설정\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'num_epochs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 학습을 에폭 수만큼 반복\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[43mnum_epochs\u001b[49m):\n\u001b[0;32m      3\u001b[0m     \n\u001b[0;32m      4\u001b[0m     \u001b[38;5;66;03m# 기울기를 0으로 설정\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m#학습 시킨 데이터와 타켓\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'num_epochs' is not defined"
     ]
    }
   ],
   "source": [
    "# 학습을 에폭 수만큼 반복\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    # 기울기를 0으로 설정\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    #학습 시킨 데이터와 타켓\n",
    "    output = model(src,tgt)\n",
    "\n",
    "    # 손실 함수 구하기\n",
    "    loss = criterion(output,tgt_label)\n",
    "\n",
    "    # 기울기 계산\n",
    "    loss.backward()\n",
    "\n",
    "    # 파라미터 업데이트하기\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\vulca/.cache\\torch\\hub\\huggingface_pytorch-transformers_main\n",
      "Using cache found in C:\\Users\\vulca/.cache\\torch\\hub\\huggingface_pytorch-transformers_main\n"
     ]
    }
   ],
   "source": [
    "model = torch.hub.load('huggingface/pytorch-transformers', 'modelForCausalLM', 'gpt2')\n",
    "tokenizer = torch.hub.load('huggingface/pytorch-transformers', 'tokenizer', 'gpt2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"I have a cat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기서 pt는 파이토치 > 토크나이저를 통해서 인풋한 텍스트를 변환\n",
    "input_ids = tokenizer.encode(input_text,return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "# 변환한 텍스트를 최대 길이 50, 1문장을 생성 할 수 있어\n",
    "output = model.generate(input_ids, max_length = 50, num_return_sequences = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  40,  423,  257, 3797,   11,  290,  314, 1101, 1016,  284, 1011, 1337,\n",
       "          286,  607,   13,  314, 1101, 1016,  284, 1011, 1337,  286,  607,   13,\n",
       "          314, 1101, 1016,  284, 1011, 1337,  286,  607,   13,  314, 1101, 1016,\n",
       "          284, 1011, 1337,  286,  607,   13,  314, 1101, 1016,  284, 1011, 1337,\n",
       "          286,  607]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I have a cat, and I'm going to take care of her. I'm going to take care of her. I'm going to take care of her. I'm going to take care of her. I'm going to take care of her\""
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 토크나이저의 생성된 아웃풋을 디코드 하여 보여주기\n",
    "# 학습할때 생성되는 특별한 토큰은 사용자에게는 필요없으니 스킵하기\n",
    "tokenizer.decode(output[0], skip_special_tokens=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
